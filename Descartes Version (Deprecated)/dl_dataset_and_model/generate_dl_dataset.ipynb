{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2503ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/descarteslabs/_dl_modules/common/workflows/arrow_serialization/context.py:57: FutureWarning: 'pyarrow.SerializationContext' is deprecated and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\n",
      "  serialization_context = pa.SerializationContext()\n",
      "/opt/conda/lib/python3.7/site-packages/descarteslabs/_dl_modules/common/workflows/arrow_serialization/context.py:58: FutureWarning: 'pyarrow.register_default_serialization_handlers' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\n",
      "  pa.register_default_serialization_handlers(serialization_context)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-contrib-python) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "#Import relevant Classes \n",
    "import descarteslabs as dl\n",
    "import descarteslabs.workflows as wf\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle as pkl\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "sys.path.append(\"../\")\n",
    "import constants\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from descarteslabs.workflows.models.exceptions import JobTimeoutError\n",
    "from gzip import compress, decompress\n",
    "from descarteslabs import Storage, Auth\n",
    "sys.path.append(\"../UI_Labeling\")\n",
    "import config\n",
    "from tqdm.notebook import tqdm\n",
    "from descarteslabs.scenes import AOI\n",
    "\n",
    "!{sys.executable} -m pip install opencv-contrib-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc73ad",
   "metadata": {},
   "source": [
    "## Editable Variables, change as needed\n",
    "## Ensure the ../UI_Labeling/config.py file has the correct user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be55f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName='updatedModel_state_sampled_slender_gerbil'\n",
    "STARTFROM=0\n",
    "TIMEOUT=600\n",
    "DATASIZE=2500\n",
    "PATHNAME='Image_Datasets/int_0_updatedModel_state_sampled_pakistan_dl_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9948ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CSV_from_dict_keys(confirmed,denied,lrimage,start):\n",
    "    columns=[]\n",
    "    count=start\n",
    "    \n",
    "    #Iterate through our image dictionary and save the image array according to the respective classification\n",
    "    print(\"Downloading Images to \"+ PATHNAME +\" folders!\")\n",
    "    for key in tqdm(lrimage.keys()):\n",
    "        folder=''\n",
    "        if key in confirmed:\n",
    "            label=1\n",
    "            folder='positives/'\n",
    "        elif key in denied:\n",
    "            label=0\n",
    "            folder='negatives/'\n",
    "        else:\n",
    "            continue;\n",
    "        info=dl.scenes.DLTile.from_key(key)\n",
    "        columns.append([folder+str(count)+'.npy',label,info.geometry,key])\n",
    "        np.save(PATHNAME+folder+str(count),lrimage[key])\n",
    "        count+=1\n",
    "        \n",
    "    #Check for existing metadata file and either add to or create using the collected column information    \n",
    "    if os.path.exists(PATHNAME+'metadata.csv'):\n",
    "        print(\"Adding to metadata.csv in \"+PATHNAME+\"!\")\n",
    "        with open(PATHNAME+'metadata.csv','a',newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(columns)\n",
    "    else:\n",
    "        print(\"Writing to metadata.csv in \"+PATHNAME+\"!\")\n",
    "        columns.insert(0,['Image','Label','Geometry','Key'])\n",
    "        with open(PATHNAME+'metadata.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a502be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10_band_images(total_dt):\n",
    "    #Initiate 10 band s2 satellite and filter by cloud_fraction\n",
    "    bands = \"coastal-aerosol blue green red red-edge red-edge-3 red-edge-4 nir swir1 swir2\"\n",
    "\n",
    "    s2=wf.ImageCollection.from_id(\n",
    "        \"esa:sentinel-2:l2a:v1\",\n",
    "        start_datetime='2018-11-01',\n",
    "        end_datetime='2019-03-01',\n",
    "    )\n",
    "\n",
    "    s2_bands = s2.pick_bands(bands)\n",
    "    s2_bands = s2_bands.filter(lambda img: img.properties[\"cloud_fraction\"]<0.1)\n",
    "    s2_bands = s2_bands.median(axis=\"images\")\n",
    "    \n",
    "    keyImageDict={}\n",
    "    if not os.path.exists(PATHNAME):\n",
    "        os.mkdir(PATHNAME)\n",
    "    if not os.path.exists(PATHNAME+'/positives'):\n",
    "        os.mkdir(PATHNAME+'/positives')\n",
    "    if not os.path.exists(PATHNAME+'/negatives'):\n",
    "        os.mkdir(PATHNAME+'/negatives')\n",
    "    \n",
    "    #Initiate getimage Function and write to the randomKeyImageDict\n",
    "    def getimage(keyPair):\n",
    "        tileKey2=keyPair\n",
    "        tile_area2=dl.scenes.DLTile.from_key(tileKey2)\n",
    "        try:\n",
    "            img_data=s2_bands.scale_values(0,255).compute(tile_area2,progress_bar=False,timeout=TIMEOUT)\n",
    "        except JobTimeoutError:\n",
    "            print('Low Res Time Out, key:',tileKey2)\n",
    "            return\n",
    "        if (img_data is not None):\n",
    "            if (img_data.ndarray is not None):\n",
    "                keyImageDict[tileKey2]=img_data.ndarray.data\n",
    "                \n",
    "    #Chunk out the images to pass to getimage to save as we go            \n",
    "    print('Getting Images!')\n",
    "    steps=math.ceil(len(total_dt)/DATASIZE)\n",
    "    for x in range(STARTFROM,steps):\n",
    "        keyImageDict.clear()\n",
    "        \n",
    "        start=x*DATASIZE\n",
    "        if (x != steps-1):\n",
    "            end=(x+1)*DATASIZE\n",
    "        else:\n",
    "            end=(len(total_dt))\n",
    "        \n",
    "        #Use threadpooling to run through each image\n",
    "        print('Group#',x, 'of',steps-1)\n",
    "        with tqdm(total=len(total_dt[start:end])) as pbar:\n",
    "            with ThreadPool(processes=64) as pool:\n",
    "                for _ in pool.imap_unordered(getimage,total_dt[start:end]):\n",
    "                    pbar.update()\n",
    "        \n",
    "        #Pass the finished key-image dict to our saving function\n",
    "        print('Saving this batch')\n",
    "        create_CSV_from_dict_keys(confirmed,denied,keyImageDict,start)\n",
    "    print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffb4d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_band_images(total_dt):\n",
    "    #Initiate 10 band s2 satellite and filter by cloud_fraction\n",
    "    bands = \"red green blue\"\n",
    "\n",
    "    ab=wf.ImageCollection.from_id(\n",
    "        \"esa:sentinel-2:l2a:v1\",\n",
    "        start_datetime='2018-11-01',\n",
    "        end_datetime='2019-03-01',\n",
    "    )\n",
    "\n",
    "    ab_bands = ab.pick_bands(bands)\n",
    "    ab_bands = ab_bands.filter(lambda img: img.properties[\"cloud_fraction\"]<0.1)\n",
    "    ab_bands = ab_bands.median(axis=\"images\")\n",
    "    \n",
    "    keyImageDict={}\n",
    "    if not os.path.exists(PATHNAME):\n",
    "        os.mkdir(PATHNAME)\n",
    "    if not os.path.exists(PATHNAME+'/positives'):\n",
    "        os.mkdir(PATHNAME+'/positives')\n",
    "    if not os.path.exists(PATHNAME+'/negatives'):\n",
    "        os.mkdir(PATHNAME+'/negatives')\n",
    "    \n",
    "    #Initiate getimage Function and write to the randomKeyImageDict\n",
    "    def getimage(keyPair):\n",
    "        tileKey2=keyPair\n",
    "        tile_area2=dl.scenes.DLTile.from_key(tileKey2)\n",
    "        aoi = AOI(tile_area2.geometry,resolution=2,crs=tile_area2.crs)\n",
    "        try:\n",
    "            img_data=ab_bands.scale_values(0,255).compute(aoi,progress_bar=False,timeout=TIMEOUT)\n",
    "        except JobTimeoutError:\n",
    "            print('Low Res Time Out, key:',tileKey2)\n",
    "            return\n",
    "        if (img_data is not None):\n",
    "            if (img_data.ndarray is not None):\n",
    "                keyImageDict[tileKey2]=img_data.ndarray.data[:,:320,:320]\n",
    "                \n",
    "    #Chunk out the images to pass to getimage to save as we go            \n",
    "    print('Getting Images!')\n",
    "    steps=math.ceil(len(total_dt)/DATASIZE)\n",
    "    for x in range(STARTFROM,steps):\n",
    "        keyImageDict.clear()\n",
    "        \n",
    "        start=x*DATASIZE\n",
    "        if (x != steps-1):\n",
    "            end=(x+1)*DATASIZE\n",
    "        else:\n",
    "            end=(len(total_dt))\n",
    "        \n",
    "        #Use threadpooling to run through each image\n",
    "        print('Group#',x, 'of',steps-1)\n",
    "        with tqdm(total=len(total_dt[start:end])) as pbar:\n",
    "            with ThreadPool(processes=64) as pool:\n",
    "                for _ in pool.imap_unordered(getimage,total_dt[start:end]):\n",
    "                    pbar.update()\n",
    "        \n",
    "        #Pass the finished key-image dict to our saving function\n",
    "        print('Saving this batch')\n",
    "        create_CSV_from_dict_keys(confirmed,denied,keyImageDict,start)\n",
    "    print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21a2a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_band_low_images(total_dt):\n",
    "    #Initiate 10 band s2 satellite and filter by cloud_fraction\n",
    "    bands = \"red green blue\"\n",
    "\n",
    "    s2=wf.ImageCollection.from_id(\n",
    "        \"esa:sentinel-2:l2a:v1\",\n",
    "        start_datetime='2018-11-01',\n",
    "        end_datetime='2019-03-01',\n",
    "    )\n",
    "\n",
    "    s2_bands = s2.pick_bands(bands)\n",
    "    s2_bands = s2_bands.filter(lambda img: img.properties[\"cloud_fraction\"]<0.1)\n",
    "    s2_bands = s2_bands.median(axis=\"images\")\n",
    "    \n",
    "    keyImageDict={}\n",
    "    if not os.path.exists(PATHNAME):\n",
    "        os.mkdir(PATHNAME)\n",
    "    if not os.path.exists(PATHNAME+'/positives'):\n",
    "        os.mkdir(PATHNAME+'/positives')\n",
    "    if not os.path.exists(PATHNAME+'/negatives'):\n",
    "        os.mkdir(PATHNAME+'/negatives')\n",
    "    \n",
    "    #Initiate getimage Function and write to the randomKeyImageDict\n",
    "    def getimage(keyPair):\n",
    "        tileKey2=keyPair\n",
    "        tile_area2=dl.scenes.DLTile.from_key(tileKey2)\n",
    "        try:\n",
    "            img_data=s2_bands.compute(tile_area2,progress_bar=False,timeout=TIMEOUT)\n",
    "        except JobTimeoutError:\n",
    "            print('Low Res Time Out, key:',tileKey2)\n",
    "            return\n",
    "        if (img_data is not None):\n",
    "            if (img_data.ndarray is not None):\n",
    "                sr=cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "                path=\"ESPCN_x4.pb\"\n",
    "                sr.readModel(path)\n",
    "                sr.setModel('espcn',4)\n",
    "                rgb=np.transpose(img_data.ndarray.data)\n",
    "                rgb=rgb-np.min(rgb)\n",
    "                rgb=rgb/np.max(rgb)\n",
    "                rgb = np.uint8(255 * rgb)\n",
    "                result=sr.upsample(rgb)\n",
    "                keyImageDict[tileKey2]=result\n",
    "                \n",
    "    #Chunk out the images to pass to getimage to save as we go            \n",
    "    print('Getting Images!')\n",
    "    steps=math.ceil(len(total_dt)/DATASIZE)\n",
    "    for x in range(STARTFROM,steps):\n",
    "        keyImageDict.clear()\n",
    "        \n",
    "        start=x*DATASIZE\n",
    "        if (x != steps-1):\n",
    "            end=(x+1)*DATASIZE\n",
    "        else:\n",
    "            end=(len(total_dt))\n",
    "        \n",
    "        #Use threadpooling to run through each image\n",
    "        print('Group#',x, 'of',steps-1)\n",
    "        with tqdm(total=len(total_dt[start:end])) as pbar:\n",
    "            with ThreadPool(processes=64) as pool:\n",
    "                for _ in pool.imap_unordered(getimage,total_dt[start:end]):\n",
    "                    pbar.update()\n",
    "        \n",
    "        #Pass the finished key-image dict to our saving function\n",
    "        print('Saving this batch')\n",
    "        create_CSV_from_dict_keys(confirmed,denied,keyImageDict,start)\n",
    "    print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be09170",
   "metadata": {},
   "source": [
    "## FOR STANDARD DATASET GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58342313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "storage=Storage(auth=Auth(client_id=config.ID,client_secret=config.SECRET))\n",
    "confirmed=pkl.loads(decompress(storage.get(datasetName+'Confirmed')))\n",
    "denied=pkl.loads(decompress(storage.get(datasetName+'Denied')))\n",
    "\n",
    "total_dt={}\n",
    "total_dt.update(denied)\n",
    "total_dt.update(confirmed)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed925ae",
   "metadata": {},
   "source": [
    "## FOR FULL COUNTRY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4afa915b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\ntotalDataset=pd.read_csv('../Predicted_Datasets/'+datasetName+'.csv').to_dict()\\nconfirmed=[]\\ndenied=[]\\nfor idx,key in enumerate(totalDataset['tile_key'].values()):\\n    if totalDataset['prediction'][idx]:\\n        confirmed.append(key)\\n    else:\\n        denied.append(key)\\ntotal_dt=confirmed.extend(denied)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "totalDataset=pd.read_csv('../Predicted_Datasets/'+datasetName+'.csv').to_dict()\n",
    "confirmed=[]\n",
    "denied=[]\n",
    "for idx,key in enumerate(totalDataset['tile_key'].values()):\n",
    "    if totalDataset['prediction'][idx]:\n",
    "        confirmed.append(key)\n",
    "    else:\n",
    "        denied.append(key)\n",
    "total_dt=confirmed.extend(denied)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25d76a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653\n",
      "1739\n",
      "2392\n"
     ]
    }
   ],
   "source": [
    "print(len(confirmed))\n",
    "print(len(denied))\n",
    "print(len(total_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956747f7",
   "metadata": {},
   "source": [
    "## GET THEM IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec54cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Images!\n",
      "Group# 0 of 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dde57456984ffaba31b702ac080f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving this batch\n",
      "Downloading Images to Image_Datasets/int_0_updatedModel_state_sampled_pakistan_dl_dataset/ folders!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872a40b46c75492093d4253179ea0312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to metadata.csv in Image_Datasets/int_0_updatedModel_state_sampled_pakistan_dl_dataset/!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_10_band_images(list(total_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b683e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c58abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
