{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47912f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-contrib-python) (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/descarteslabs/_dl_modules/common/workflows/arrow_serialization/context.py:57: FutureWarning: 'pyarrow.SerializationContext' is deprecated and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\n",
      "  serialization_context = pa.SerializationContext()\n",
      "/opt/conda/lib/python3.7/site-packages/descarteslabs/_dl_modules/common/workflows/arrow_serialization/context.py:58: FutureWarning: 'pyarrow.register_default_serialization_handlers' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\n",
      "  pa.register_default_serialization_handlers(serialization_context)\n"
     ]
    }
   ],
   "source": [
    "# try the imports\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from descarteslabs import Storage, Auth\n",
    "%pip install torchsummary\n",
    "from torchsummary import summary\n",
    "sys.path.append(\"../\")\n",
    "import constants\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('../UI_Labeling')\n",
    "import config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "!{sys.executable} -m pip install opencv-contrib-python\n",
    "import cv2\n",
    "import json\n",
    "import torchvision.transforms as T\n",
    "from torchvision import transforms\n",
    "\n",
    "from shapely.wkt import loads\n",
    "import geopandas\n",
    "from descarteslabs.scenes import DLTile\n",
    "import geojson\n",
    "import descarteslabs.workflows as wf\n",
    "from ipyleaflet import GeoJSON\n",
    "import json\n",
    "from descarteslabs.scenes import AOI\n",
    "from descarteslabs.workflows.models.exceptions import JobTimeoutError\n",
    "import torch\n",
    "from scipy import signal\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f2b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class Combo_BI_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for country/ies DL Sentinel2 10-channel imagery.\n",
    "    \"\"\"\n",
    "    def __init__(self, DS_Name, balance=False, limit=0, mode=\"full\",transform=None):\n",
    "        super(Combo_BI_Dataset, self).__init__()\n",
    "        self.df=pd.DataFrame()\n",
    "        self.upsampled=True\n",
    "        self.transform=transform\n",
    "        \n",
    "        def add_dataset(dataset_name):\n",
    "            df=pd.read_csv(\"Image_Datasets/\"+dataset_name+'/metadata.csv')\n",
    "            if dataset_name == 'bangladesh_dl_dataset' or dataset_name == 'india_dl_dataset':\n",
    "                self.upsampled=False\n",
    "                \n",
    "            positives=0\n",
    "            negatives=0\n",
    "            if balance:\n",
    "                confirmed={'Image':{},'Label':{},'Geometry':{}}\n",
    "                denied={'Image':{},'Label':{},'Geometry':{}}\n",
    "                for idx in df['Label'].keys():\n",
    "                    if df['Label'][idx]:\n",
    "                        positives+=1\n",
    "                        confirmed['Image'][idx]=df['Image'][idx]\n",
    "                        confirmed['Label'][idx]=df['Label'][idx]\n",
    "                        confirmed['Geometry'][idx]=df['Geometry'][idx]\n",
    "                    else:\n",
    "                        negatives+=1\n",
    "                        denied['Image'][idx]=df['Image'][idx]\n",
    "                        denied['Label'][idx]=df['Label'][idx]\n",
    "                        denied['Geometry'][idx]=df['Geometry'][idx]\n",
    "                print('positives:',positives,'negatives:',negatives)    \n",
    "                if limit:\n",
    "                    random.seed(constants.RANDOM_STATE)\n",
    "                    sample_keys=random.sample(list(confirmed['Image'].keys()),min([len(confirmed['Image']),limit]))\n",
    "                    confirmed={'Image':{},'Label':{},'Geometry':{}}\n",
    "                    for idx in sample_keys:\n",
    "                        confirmed['Image'][idx]=df['Image'][idx]\n",
    "                        confirmed['Label'][idx]=df['Label'][idx]\n",
    "                        confirmed['Geometry'][idx]=df['Geometry'][idx]\n",
    "                random.seed(constants.RANDOM_STATE)\n",
    "                sample_keys=random.sample(list(denied['Image'].keys()),min([len(denied['Image']),math.ceil((len(confirmed['Image'])*3))]))\n",
    "                denied={'Image':{},'Label':{},'Geometry':{}}\n",
    "                for idx in sample_keys:\n",
    "                    denied['Image'][idx]=df['Image'][idx]\n",
    "                    denied['Label'][idx]=df['Label'][idx]\n",
    "                    denied['Geometry'][idx]=df['Geometry'][idx]\n",
    "                confirmed['Image'].update(denied['Image'])\n",
    "                confirmed['Label'].update(denied['Label'])\n",
    "                confirmed['Geometry'].update(denied['Geometry'])\n",
    "                df=pd.DataFrame().from_dict(confirmed)\n",
    "            #\"\"\"\n",
    "            df[\"Image\"]='Image_Datasets/'+dataset_name+'/'+df[\"Image\"]\n",
    "            df=df[['Image','Label']]\n",
    "            return df\n",
    "            \n",
    "    # ADD DATASETS HERE FOR TRAINING\n",
    "        \n",
    "        self.df = pd.concat([self.df,add_dataset(DS_Name)])\n",
    "        \n",
    "    # END EDITABLES\n",
    "        \n",
    "        self.df = self.df.sample(frac=1, random_state=constants.RANDOM_STATE)\n",
    "        \n",
    "        if mode == \"tiny\":\n",
    "            self.df = self.df.sample(frac=.05, random_state=constants.RANDOM_STATE)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name, label = self.df.iloc[idx]\n",
    "        if not self.upsampled:\n",
    "            img = np.load(file_name)[1:4]\n",
    "            sr=cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "            path=\"ESPCN_x4.pb\"\n",
    "            sr.readModel(path)\n",
    "            sr.setModel('espcn',4)\n",
    "            rgb=np.transpose(img)\n",
    "            rgb=rgb-np.min(rgb)\n",
    "            rgb=rgb/np.max(rgb)\n",
    "            rgb = np.uint8(255 * rgb)\n",
    "            result=sr.upsample(rgb)\n",
    "            img = torch.Tensor(result/256).float().to(device)\n",
    "        else:\n",
    "            img = torch.Tensor(np.load(file_name)/256).float().to(device)\n",
    "        label = torch.tensor([label]).to(device)\n",
    "        \n",
    "        if self.transform:\n",
    "            img=self.transform(np.transpose(img,(2,0,1)))\n",
    "            img= np.transpose(img,(1,2,0))*256\n",
    "        else:\n",
    "            img=img*256\n",
    "        img=img.int()\n",
    "        return img, label, file_name\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformList=None\n",
    "\"\"\"\n",
    "transformList=transforms.RandomApply(torch.nn.ModuleList([\n",
    "    transforms.ColorJitter(hue=.1,contrast=.25,saturation=.5,brightness=.5),\n",
    "    transforms.RandomAffine(15),\n",
    "]),p=.25)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "train_dset=None\n",
    "val_dset=None\n",
    "#Commend or uncomment for datasets you'd like to add.\n",
    "dataset.append(Combo_BI_Dataset('Upsampled_bangladesh_dl_dataset',True,858))\n",
    "dataset.append(Combo_BI_Dataset('Upsampled_india_dl_dataset',True,858))\n",
    "dataset.append(Combo_BI_Dataset('v2_Upsampled_high_sampled_india_dl_dataset',True,858,transform=transformList))\n",
    "dataset.append(Combo_BI_Dataset('v2_Upsampled_updatedModel_state_sampled_pakistan_dl_dataset',True,858,transform=transformList))\n",
    "dataset.append(Combo_BI_Dataset('v2_Upsampled_updatedModel_state_sampled_nepal_dl_dataset',True,858,transform=transformList))\n",
    "\n",
    "for dset in dataset:\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        dset, \n",
    "        [len(dset)*8//10, len(dset)-len(dset)*8//10], # 80-10% split\n",
    "        generator=torch.Generator().manual_seed(constants.RANDOM_STATE)\n",
    "    )\n",
    "    if train_dset is None and val_dset is None:\n",
    "        train_dset=train\n",
    "        val_dset=val\n",
    "    else:\n",
    "        train_dset=torch.utils.data.ConcatDataset([train_dset,train])\n",
    "        val_dset=torch.utils.data.ConcatDataset([val_dset,val])\n",
    "print(f\"{len(train_dset)} training examples and {len(val_dset)} validation examples.\")\n",
    "train_loader = DataLoader(train_dset, batch_size=64,shuffle=True)\n",
    "val_loader = DataLoader(val_dset, batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "\n",
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self, num_channels, image_width, device=None, pretrained=False):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.device = device        \n",
    "        self.resnet = resnet18(pretrained=pretrained)\n",
    "        self.resnet.conv1 = torch.nn.Conv2d(num_channels, image_width, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, 256).to(device)\n",
    "        self.final_fc = torch.nn.Linear(256, 1).to(device)\n",
    "        \n",
    "        self.resnet = self.resnet.to(device)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        resnet_output = self.resnet(x)\n",
    "        outputs = self.final_fc(resnet_output)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "def load_checkpoint(model_checkpoint, model, device, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loads a pretrained checkpoint to continue training\n",
    "    model_checkpoint: Path of the model_checkpoint that ends with .pth\n",
    "    model: model to load to\n",
    "    device: devide to load on (gpu/cpu)\n",
    "    optimizer (optional): optimize to load state to\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_checkpoint, map_location=device)\n",
    "    print('Loaded best state dict from epoch: {}'.format(checkpoint[\"epoch\"]))\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "model_name = \"Upsampled_AugmentedTrain_bipn_v7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logdir = \"model_log_dl/\"+model_name+\"_50_training_steps/\"\n",
    "net = Resnet(3,64, device=device)\n",
    "net = load_checkpoint(f\"{logdir}/checkpoints/best_dl_best.pth\", \n",
    "                                 net, \n",
    "                                 device)\n",
    "\n",
    "finalconv_name='layer4'\n",
    "torch.nn.Upsample()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf07cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f4bd3a175d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "    \n",
    "net._modules.get('resnet')._modules.get(finalconv_name).register_forward_hook(hook_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da996ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "weight_softmax = [np.dot(np.squeeze(params[-2].data.numpy()),(np.squeeze(params[-4].data.numpy())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f43c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b815e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnOriginalCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    labeled=[]\n",
    "    nr_objects=[]\n",
    "    centers=[]\n",
    "    for idx in class_idx:\n",
    "        cam = (weight_softmax[idx]).dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        \n",
    "        #normalize\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        \n",
    "        #print(cam)\n",
    "        #calculate mean and convolute using large mask\n",
    "        \"\"\"\n",
    "        mean = np.mean(cam[np.nonzero(cam)])\n",
    "        cam = cam-mean\n",
    "        cam = np.clip(cam,0,None)\n",
    "        cam = cv2.GaussianBlur(cam,(5,5),0)\n",
    "        \n",
    "        #convolve=signal.convolve2d(cam,mask1)\n",
    "        #cam=convolve\n",
    "        #\"\"\"\n",
    "        \n",
    "        #calculate mean and convolute using medium mask\n",
    "        \"\"\"\n",
    "        mean = np.mean(cam[np.nonzero(cam)])\n",
    "        cam = cam-mean\n",
    "        cam = np.clip(cam,0,None)\n",
    "        #cam = cv2.GaussianBlur(cam,(3,3),0)\n",
    "        \n",
    "        #convolve=signal.convolve2d(cam,mask3)\n",
    "        #cam=convolve\n",
    "        #\"\"\"\n",
    "        \n",
    "        #calculate mean and convolute using small mask\n",
    "        #\"\"\"\n",
    "        #mean = np.mean(cam[np.nonzero(cam)])\n",
    "        #cam = cam-mean\n",
    "        #cam = np.clip(cam,0,None)\n",
    "        #cam = cv2.GaussianBlur(cam,(3,3),0)\n",
    "        #\"\"\"\n",
    "        \n",
    "        #calculate mean and send the final cam\n",
    "        \"\"\"\n",
    "        mean = np.mean(cam[np.nonzero(cam)])\n",
    "        cam = cam-mean\n",
    "        #cam = np.clip(cam,0,None)\n",
    "        #mean = np.mean(cam[np.nonzero(cam)])\n",
    "        #cam = cam-mean\n",
    "        #\"\"\"\n",
    "        \n",
    "        cam_img = np.clip(cam,0,None)\n",
    "        cam_img=cv2.resize(cam_img, size_upsample)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cam_img)\n",
    "        labeled1,nr_objects1=ndimage.label(output_cam)\n",
    "        labeled.append(labeled1)\n",
    "        nr_objects.append(nr_objects1)\n",
    "        centers1=ndimage.measurements.center_of_mass(output_cam[-1].squeeze(),labeled[-1].squeeze(),range(1,nr_objects1+1))\n",
    "        centers.append(centers1)\n",
    "    return output_cam,labeled,nr_objects,centers\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    labeled=[]\n",
    "    nr_objects=[]\n",
    "    centers=[]\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        \n",
    "        #normalize\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "        \n",
    "        #print(cam)\n",
    "        #calculate mean and convolute using large mask\n",
    "        #\"\"\"\n",
    "        mean = np.mean(cam[np.nonzero(cam)])\n",
    "        cam = cam-mean\n",
    "        cam = np.clip(cam,0,None)\n",
    "        cam = cv2.GaussianBlur(cam,(5,5),0)\n",
    "        \n",
    "        #convolve=signal.convolve2d(cam,mask1)\n",
    "        #cam=convolve\n",
    "        #\"\"\"\n",
    "        \n",
    "        #calculate mean and convolute using medium mask\n",
    "        #\"\"\"\n",
    "        mean = np.mean(cam[np.nonzero(cam)])\n",
    "        cam = cam-mean\n",
    "        cam = np.clip(cam,0,None)\n",
    "        #cam = cv2.GaussianBlur(cam,(3,3),0)\n",
    "        \n",
    "        #convolve=signal.convolve2d(cam,mask3)\n",
    "        #cam=convolve\n",
    "        #\"\"\"\n",
    "        \n",
    "        #calculate mean and convolute using small mask\n",
    "        #\"\"\"\n",
    "        #mean = np.mean(cam[np.nonzero(cam)])\n",
    "        #cam = cam-mean\n",
    "        #cam = np.clip(cam,0,None)\n",
    "        #cam = cv2.GaussianBlur(cam,(3,3),0)\n",
    "        #\"\"\"\n",
    "        \n",
    "        #calculate mean and send the final cam\n",
    "        #\"\"\"\n",
    "        mean = np.mean(cam[np.nonzero(cam)])\n",
    "        cam = cam-mean\n",
    "        #cam = np.clip(cam,0,None)\n",
    "        #mean = np.mean(cam[np.nonzero(cam)])\n",
    "        #cam = cam-mean\n",
    "        #\"\"\"\n",
    "        \n",
    "        cam_img = np.clip(cam,0,None)\n",
    "        cam_img=cv2.resize(cam_img, size_upsample)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cam_img)\n",
    "        labeled1,nr_objects1=ndimage.label(output_cam)\n",
    "        labeled.append(labeled1)\n",
    "        nr_objects.append(nr_objects1)\n",
    "        centers1=ndimage.measurements.center_of_mass(output_cam[-1].squeeze(),labeled[-1].squeeze(),range(1,nr_objects1+1))\n",
    "        centers.append(centers1)\n",
    "    return output_cam,labeled,nr_objects,centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1daf2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = \"red green blue\"\n",
    "s2=wf.ImageCollection.from_id(\n",
    "    \"esa:sentinel-2:l2a:v1\",\n",
    "    start_datetime='2018-11-01',\n",
    "    end_datetime='2019-03-01',\n",
    ")\n",
    "\n",
    "s2_bands = s2.pick_bands(bands)\n",
    "s2_bands = s2_bands.filter(lambda img: img.properties[\"cloud_fraction\"]<0.1)\n",
    "s2_bands = s2_bands.median(axis=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f1189a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1fd2aab6c84d85b961ac9690014437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val False: [1098.  188.   78.   42.   35.   25.   12.    8.   13.   11.    5.    6.\n",
      "    2.    5.    5.    5.    4.    0.    5.    7.]\n",
      "Val True: [ 34.  19.  20.  15.  10.  13.   7.   9.  14.   9.  14.  11.  18.  17.\n",
      "  19.  18.  32.  32.  65. 253.]\n"
     ]
    }
   ],
   "source": [
    "#Show Distribution of Prediction Probabilities\n",
    "#\"\"\"\n",
    "plt.clf()\n",
    "distribution=[]\n",
    "ef=np.zeros(20)\n",
    "et=np.zeros(20)\n",
    "fpositives=[]\n",
    "fnegatives=[]\n",
    "temp_blobs=[]\n",
    "\n",
    "for x in tqdm(range(len(val_dset))):\n",
    "    img=val_dset[x][0]\n",
    "    img_variable = Variable(img.unsqueeze(0).float())\n",
    "    logit = net(img_variable).detach().numpy()[0]\n",
    "    valuation=val_dset[x][1].detach().numpy()\n",
    "    ix=math.floor(logit/.05)\n",
    "    if ix >= 20:\n",
    "        ix = 19\n",
    "    if valuation:\n",
    "        et[ix]+=1\n",
    "    else:\n",
    "        ef[ix]+=1\n",
    "    \n",
    "    if np.logical_xor(logit>=.9,valuation==1.0):\n",
    "        if logit>=.9:\n",
    "            fpositives.append([img,logit,valuation,x])\n",
    "        else:\n",
    "            fnegatives.append([img,logit,valuation,x])\n",
    "            \n",
    "temp_blobs=features_blobs\n",
    "stack=np.vstack([ef,et])\n",
    "fig, ax = plt.subplots()\n",
    "ax.stackplot(np.arange(0,100,5), stack)\n",
    "plt.legend(['Val False', 'Val True'])\n",
    "plt.show()\n",
    "print('Val False:',stack[0])\n",
    "print('Val True:',stack[1])\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4258a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negativse: 311\n",
      "False Positives: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.sort_values except for the argument 'by' will be keyword-only\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.sort_values except for the argument 'by' will be keyword-only\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "fneg=pd.DataFrame(fnegatives,columns=['img','logit','valuation','x'])\n",
    "fneg=fneg.sort_values(['logit'],0)\n",
    "\n",
    "fpos=pd.DataFrame(fpositives,columns=['img','logit','valuation','x'])\n",
    "fpos=fpos.sort_values(['logit'],0)\n",
    "\n",
    "fnsamples=fneg.head(min([len(fneg),10]))\n",
    "fntailsamples=None\n",
    "\n",
    "fpsamples=fpos.head(min([len(fpos),10]))\n",
    "fptailsamples=None\n",
    "\n",
    "if len(fneg)>10:\n",
    "    fntailsamples=fneg.tail(min([10,len(fneg)-10]))\n",
    "\n",
    "if len(fpos)>10:\n",
    "    fptailsamples=fpos.tail(min([10,len(fpos)-10]))\n",
    "print('False Negativse:',len(fnegatives))\n",
    "print('False Positives:',len(fpositives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a7a724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c7bb0050f94d8eb1b530fa884797c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785f668e319d456e8908bc26f128f7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743f48ee90ca45a291aaf19d1b40971c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e45bc50d44e41c78d7d54d907cc31fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "title=['Lowest Prob False Positives','Highest Prob False Positives','Lowest Prob False Negatives','Highest Prob False Negatives']\n",
    "for y,section in enumerate([fpsamples,fptailsamples,fnsamples,fntailsamples]):\n",
    "    if section is not None and len(section)>0:\n",
    "        lfig= plt.figure(title[y])\n",
    "        subfigs = lfig.subfigures(3,4)\n",
    "        for x,idx in enumerate(section.index):\n",
    "            sample=section.loc[idx].values.flatten()\n",
    "\n",
    "            CAMs,labeled,nr_objects,centers = returnCAM(temp_blobs[sample[3]], weight_softmax, [0])\n",
    "            oCAMs,olabeled,onr_objects,ocenters = returnOriginalCAM(temp_blobs[sample[3]],weight_softmax,[0])\n",
    "            subplot=subfigs[math.floor(x/4)][x%4].subplots(2,2)\n",
    "            subfigs[math.floor(x/4)][x%4].suptitle('Pred:'+str(sample[1])+' Label:'+str(sample[2]))\n",
    "            subplot[0,0].imshow(oCAMs[-1])\n",
    "            subplot[0,0].axis('off')\n",
    "            subplot[0,1].imshow(CAMs[-1])\n",
    "            subplot[0,1].axis('off')    \n",
    "            subplot[1,0].imshow(np.transpose(labeled[-1],(1,2,0)))\n",
    "            subplot[1,0].axis('off')\n",
    "            subplot[1,1].imshow(np.transpose(sample[0].int(),(1,2,0)))\n",
    "            subplot[1,1].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599c681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
