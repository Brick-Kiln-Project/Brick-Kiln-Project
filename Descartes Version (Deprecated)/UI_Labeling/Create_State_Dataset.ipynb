{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296b8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "import pandas as pd\n",
    "import shapely.wkt\n",
    "from shapely import geometry\n",
    "import descarteslabs as dl\n",
    "import pickle as pkl\n",
    "import geopandas as gpd\n",
    "from random import sample\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870e5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName='happy_cow'\n",
    "country='nepal'\n",
    "stateSeparatedFile='updatedModel_state_sampled_nepal'\n",
    "datasetFile='updatedModel_state_sampled_happy_cow'\n",
    "maxKilnStateSample=500\n",
    "maxNonKilnStateSample=250\n",
    "minNonKilnStateSample=50\n",
    "randomSample=500\n",
    "oversampleStates=['F.A.T.A','N.W.F.P','Punjab']\n",
    "oversampleModifier=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c34d4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a63ec4e76fd4e018cd4cc569fd2bf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "#Load Datasets\n",
    "highDataset={'tile_key':{},'geometry':{},'prediction':{},'area':{}}\n",
    "restDataset={'tile_key':{},'geometry':{},'prediction':{},'area':{}}\n",
    "totalDataset=pd.read_csv('../Predicted_Datasets/'+datasetName+'_results.csv').to_dict()\n",
    "highcount=0\n",
    "lowcount=0\n",
    "for idx in tqdm(totalDataset['prediction']):\n",
    "    if totalDataset['prediction'][idx] >= .99:\n",
    "        highDataset['tile_key'][highcount]=totalDataset['tile_key'][idx]\n",
    "        highDataset['geometry'][highcount]=totalDataset['geometry'][idx]\n",
    "        highDataset['prediction'][highcount]=totalDataset['prediction'][idx]\n",
    "        highcount+=1\n",
    "    else:\n",
    "        restDataset['tile_key'][lowcount]=totalDataset['tile_key'][idx]\n",
    "        restDataset['geometry'][lowcount]=totalDataset['geometry'][idx]\n",
    "        restDataset['prediction'][lowcount]=totalDataset['prediction'][idx]\n",
    "        lowcount+=1\n",
    "print(highcount)\n",
    "with open('../GeoJSONS/Brick_Beltv2.geojson') as f:\n",
    "    brick_belt = geojson.load(f) \n",
    "with open('../GeoJSONS/Bangladesh_States.geojson') as f:\n",
    "    states=geojson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f763a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create shapely brick belt\n",
    "shapelyBelt=geometry.shape(brick_belt['features'][0]['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e102a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare to make a geodataframe from the geometry of the indian states, also take the time to calculate non-belt max area\n",
    "shapelyStates={}\n",
    "togpd=[]\n",
    "names=[]\n",
    "within={}\n",
    "max_area=0\n",
    "for x in states['features']:\n",
    "    if shapelyBelt.intersects(geometry.shape(x['geometry'])):\n",
    "        within = shapelyBelt.intersection(geometry.shape(x['geometry'])).area/geometry.shape(x['geometry']).area >= .25\n",
    "    else:\n",
    "        within=False\n",
    "    shapelyStates[x['properties']['NAME_1']]={\n",
    "        'geometry':geometry.shape(x['geometry']),\n",
    "        'within':within,\n",
    "        'tiles':[]\n",
    "    }\n",
    "    if not shapelyStates[x['properties']['NAME_1']]['within']:\n",
    "        if max_area<shapelyStates[x['properties']['NAME_1']]['geometry'].area:\n",
    "            max_area=shapelyStates[x['properties']['NAME_1']]['geometry'].area\n",
    "    togpd.append(geometry.shape(x['geometry']))\n",
    "    names.append(x['properties']['NAME_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91e0582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(togpd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94bd5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create state geodataframe\n",
    "gdf = gpd.GeoDataFrame(data={'name':names}, crs='epsg:4326', geometry=togpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be905f",
   "metadata": {},
   "source": [
    "## UNCOMENT THE TWO BOXES BELOW IF YOUR TILE DATA NEEDS TO BE REASSESED OR YOU NEED TO MAKE A NEW POINT-STATE GEOMETRY\n",
    "## (DON'T FORGET TO CHANGE ANS FILE NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac1f3ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d289dbc0c54095af5c45f031e25035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\"\"\"\n",
    "#Iterate through all of our keys and obtain the geometric center and the respective tile_keys\n",
    "centers=[]\n",
    "tile_keys=[]\n",
    "for x in tqdm(highDataset['tile_key'].keys()):\n",
    "    centers.append(dl.scenes.DLTile.from_key(highDataset['tile_key'][x]).geometry.centroid)\n",
    "    tile_keys.append(highDataset['tile_key'][x])\n",
    "    \n",
    "#Create the tile geodataframe from the center points \n",
    "gdfp = gpd.GeoDataFrame(data={'tile_key':tile_keys,'idx':range(len(tile_keys))},crs='epsg:4326', geometry=centers)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46384ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POINT (81.07496437662 28.15667017686067)\n"
     ]
    }
   ],
   "source": [
    "print(centers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06683fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#Create and save the joined point-state geometry as a csv\n",
    "ans = gpd.tools.sjoin(gdfp, gdf, predicate=\"within\", how='left')\n",
    "ans.drop('geometry',axis=1).to_csv('State_Separated_Datasets/'+stateSeparatedFile+'.csv')\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff7825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ans\n",
    "ans=pd.read_csv('State_Separated_Datasets/'+stateSeparatedFile+'.csv').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f549d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d2acc167444ab7940ba585f79f6d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Iterate through each tile once more and add each tile to it's respective state list\n",
    "stateTiles={}\n",
    "for x in shapelyStates.keys():\n",
    "    stateTiles[x]=[]\n",
    "for x in tqdm(range(len(ans['tile_key']))):\n",
    "    if type(ans['name'][x])==str:\n",
    "        stateTiles[ans['name'][x]].append([ans['tile_key'][x],ans['idx'][x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb9b379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Province No. 1!\n",
      "Inside Brick Belt!\n",
      "Sampling 27 tiles!\n",
      "Saving!\n",
      "Working on Province No. 2!\n",
      "Inside Brick Belt!\n",
      "Sampling 53 tiles!\n",
      "Saving!\n",
      "Working on Bagmati Pradesh!\n",
      "Inside Brick Belt!\n",
      "Sampling 6 tiles!\n",
      "Saving!\n",
      "Working on Gandaki Pradesh!\n",
      "Inside Brick Belt!\n",
      "Sampling 5 tiles!\n",
      "Saving!\n",
      "Working on Province No. 5!\n",
      "Inside Brick Belt!\n",
      "Sampling 13 tiles!\n",
      "Saving!\n",
      "Working on Karnali Pradesh!\n",
      "Inside Brick Belt!\n",
      "Sampling 0 tiles!\n",
      "Saving!\n",
      "Working on Sudurpashchim Pradesh!\n",
      "Inside Brick Belt!\n",
      "Sampling 0 tiles!\n",
      "Saving!\n",
      "Sampling randomly!\n",
      "Sampling 500 tiles!\n",
      "Saving!\n"
     ]
    }
   ],
   "source": [
    "with open('../Predicted_Datasets/'+datasetFile+'_results.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([['tile_key','geometry','prediction','area']])\n",
    "    \n",
    "#Iterate through each state and sample respective amounts per state, then save it to the sampled csv.    \n",
    "for x in shapelyStates.keys():\n",
    "    print('Working on '+x+'!')\n",
    "    if x in oversampleStates:\n",
    "        print('Oversampling!')\n",
    "        modifier=oversampleModifier\n",
    "    else:\n",
    "        modifier=1\n",
    "    if shapelyStates[x]['within']:\n",
    "        print('Inside Brick Belt!')\n",
    "        samplesize=min([len(stateTiles[x]),round(maxKilnStateSample*modifier)])\n",
    "    else:\n",
    "        print('Outside Brick Belt!')\n",
    "        cur_area=shapelyStates[x]['geometry'].area\n",
    "        samplesize=min([round(maxNonKilnStateSample*cur_area/max_area*modifier),len(stateTiles[x])])\n",
    "        if round(minNonKilnStateSample*modifier) > len(stateTiles[x]):\n",
    "            samplesize=len(stateTiles[x])\n",
    "        else:\n",
    "            samplesize=max([round(minNonKilnStateSample*modifier),samplesize])\n",
    "    print('Sampling '+str(samplesize)+' tiles!')\n",
    "    shapelyStates[x]['tiles']=sample(stateTiles[x],samplesize)\n",
    "    rows=[]\n",
    "    print('Saving!')\n",
    "    for y in range(len(shapelyStates[x]['tiles'])):\n",
    "        rows.append([highDataset['tile_key'][shapelyStates[x]['tiles'][y][1]],highDataset['geometry'][shapelyStates[x]['tiles'][y][1]],highDataset['prediction'][shapelyStates[x]['tiles'][y][1]],x])\n",
    "    with open('../Predicted_Datasets/'+datasetFile+'_results.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "if randomSample:\n",
    "    print('Sampling randomly!')\n",
    "    sampledkeys=sample(list(restDataset['tile_key'].keys()),min([len(restDataset['tile_key'].keys()),randomSample]))\n",
    "    print('Sampling',len(sampledkeys),'tiles!')\n",
    "    rows=[]\n",
    "    print('Saving!')\n",
    "    for x in sampledkeys:\n",
    "        rows.append([restDataset['tile_key'][x],restDataset['geometry'][x],restDataset['prediction'][x],'random'])\n",
    "    with open('../Predicted_Datasets/'+datasetFile+'_results.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef37656",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledDataset=pd.read_csv('../Predicted_Datasets/'+datasetFile+'_results.csv').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9644c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/descarteslabs/_dl_modules/common/workflows/arrow_serialization/context.py:57: FutureWarning: 'pyarrow.SerializationContext' is deprecated and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\n",
      "  serialization_context = pa.SerializationContext()\n",
      "/opt/conda/lib/python3.7/site-packages/descarteslabs/_dl_modules/common/workflows/arrow_serialization/context.py:58: FutureWarning: 'pyarrow.register_default_serialization_handlers' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\n",
      "  pa.register_default_serialization_handlers(serialization_context)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feef4606de4457baaee58dfeafe8619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import descarteslabs.workflows as wf\n",
    "from ipyleaflet import GeoJSON\n",
    "m = wf.interactive.MapApp()\n",
    "for x in tqdm(sampledDataset['area'].keys()):\n",
    "    #if sampledDataset['area'][x]=='Andhra Pradesh':\n",
    "        subtilegjs=GeoJSON(\n",
    "           data=geojson.FeatureCollection([dl.scenes.DLTile.from_key(sampledDataset['tile_key'][x]).__geo_interface__]),\n",
    "           style={\"color\":\"red\",\n",
    "                  \"fillOpacity\":0}\n",
    "        )\n",
    "        m.add_layer(subtilegjs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f288e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa0b045004d47fb8aa9cf335e159448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "\n",
       "`ipyleaflet` and/or `ipywidgets` Jupyter extensions are not installed! (or you're not in a Jupyter notebook.)\n",
       "To install for JupyterLab, run this in a cell:\n",
       "    !jupyter labextension install jupyter-leaflet @jupyter-widgets/jupyterlab-manager\n",
       "To install for plain Jupyter Notebook, run this in a cell:\n",
       "    !jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
       "Then, restart the kernel and refresh the webpage.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e306522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e58cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
